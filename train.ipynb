{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dataroot= 'D:/VILAB/datasets/for_inpainting/train/RAFDB' #image dataroot\n",
    "        self.maskroot= 'D:/VILAB/datasets/for_inpainting/mask/mask_224'#mask dataroot\n",
    "        self.batchSize= 1# Need to be set to 1\n",
    "        self.fineSize=224 # image size\n",
    "        self.input_nc=3  # input channel size for first stage\n",
    "        self.input_nc_g=6 # input channel size for second stage\n",
    "        self.output_nc=3# output channel size\n",
    "        self.ngf=64 # inner channel\n",
    "        self.ndf=64# inner channel\n",
    "        self.which_model_netD='basic' # patch discriminator\n",
    "        self.which_model_netF='feature'# feature patch discriminator\n",
    "        self.which_model_netG='unet_csa'# seconde stage network\n",
    "        self.which_model_netP='unet_256'# first stage network\n",
    "        self.triple_weight=1\n",
    "        self.name='irregular_mask_inpainting'\n",
    "        self.n_layers_D='3' # network depth\n",
    "        self.gpu_ids=[0]\n",
    "        self.model='csa_net'\n",
    "        self.checkpoints_dir='checkpoints/' #\n",
    "        self.norm='instance'\n",
    "        self.fixed_mask=1\n",
    "        self.use_dropout=False\n",
    "        self.init_type='normal'\n",
    "        self.mask_type='random'\n",
    "        self.lambda_A=100\n",
    "        self.threshold=5/16.0\n",
    "        self.stride=1\n",
    "        self.shift_sz=1 # size of feature patch\n",
    "        self.mask_thred=1\n",
    "        self.bottleneck=512\n",
    "        self.gp_lambda=10.0\n",
    "        self.ncritic=5\n",
    "        self.constrain='MSE'\n",
    "        self.strength=1\n",
    "        self.init_gain=0.02\n",
    "        self.cosis=1\n",
    "        self.gan_type='lsgan'\n",
    "        self.gan_weight=0.2\n",
    "        self.overlap=4\n",
    "        self.skip=0\n",
    "        self.display_freq=1000\n",
    "        self.print_freq=50\n",
    "        self.save_latest_freq=5000\n",
    "        self.save_epoch_freq=2\n",
    "        self.continue_train=True\n",
    "        self.epoch_count=1\n",
    "        self.phase='train'\n",
    "        self.which_epoch='20'\n",
    "        self.niter=20\n",
    "        self.niter_decay=100\n",
    "        self.beta1=0.5\n",
    "        self.lr=0.0002\n",
    "        self.lr_policy='lambda'\n",
    "        self.lr_decay_iters=50\n",
    "        self.isTrain=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAN\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12271\n",
      "csa_net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\GAN\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "D:\\anaconda\\envs\\GAN\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VILAB\\CSA_idea\\models\\networks.py:54: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, 0.0, gain)\n",
      "D:\\VILAB\\CSA_idea\\models\\networks.py:64: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias.data, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "Loading pre-trained network!\n",
      "---------- Networks initialized -------------\n",
      "model [CSAModel] was created\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from util.data_load import Data_load\n",
    "from models.models import create_model\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "opt = Option()\n",
    "transform_mask = transforms.Compose(\n",
    "    [transforms.Resize((256,256)),\n",
    "     transforms.ToTensor(),\n",
    "    ])\n",
    "transform = transforms.Compose(\n",
    "    [#transforms.RandomHorizontalFlip(),\n",
    "     transforms.Resize((256,256)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)])\n",
    "\n",
    "dataset_train = Data_load(opt.dataroot, opt.maskroot, transform, transform_mask)\n",
    "iterator_train = (data.DataLoader(dataset_train, batch_size=1,shuffle=True))\n",
    "print(len(dataset_train))\n",
    "model = create_model(opt)\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\VILAB\\CSA_idea\\models\\CSA.py:121: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1240.)\n",
      "  self.input_A.narrow(1,0,1).masked_fill_(self.mask_global, 2*123.0/255.0 - 1.0)\n",
      "D:\\VILAB\\CSA_idea\\models\\CSA.py:122: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1240.)\n",
      "  self.input_A.narrow(1,1,1).masked_fill_(self.mask_global, 2*104.0/255.0 - 1.0)\n",
      "D:\\VILAB\\CSA_idea\\models\\CSA.py:123: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1240.)\n",
      "  self.input_A.narrow(1,2,1).masked_fill_(self.mask_global, 2*117.0/255.0 - 1.0)\n",
      "D:\\VILAB\\CSA_idea\\models\\CSA.py:138: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1240.)\n",
      "  self.Unknowregion=self.un.data.masked_fill_(self.inv_ex_mask, 0)\n",
      "D:\\VILAB\\CSA_idea\\models\\CSA.py:139: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1240.)\n",
      "  self.knownregion=self.real_A.data.masked_fill_(self.ex_mask, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([512, 32, 32])\n",
      "torch.Size([1, 1024, 32, 32])\n",
      "tensor([[[[-2.3689e+01, -5.7573e+01, -9.8838e+01,  ..., -3.5735e+01,\n",
      "           -2.2026e+01, -1.4230e+01],\n",
      "          [-5.2492e+00, -4.0623e+01, -8.1754e+01,  ..., -2.9330e+01,\n",
      "           -3.2100e+01, -1.8822e+01],\n",
      "          [ 1.8030e+01, -3.1485e+00, -2.6861e+01,  ..., -2.2342e+01,\n",
      "           -2.8734e+01, -2.4852e+01],\n",
      "          ...,\n",
      "          [-3.2135e+00, -2.4895e+01, -3.7998e+01,  ..., -1.8315e+01,\n",
      "           -7.2629e+01, -9.1551e+01],\n",
      "          [-1.8162e+00, -8.6395e+00, -3.6550e+00,  ..., -4.5231e+01,\n",
      "           -4.8756e+01, -7.1420e+01],\n",
      "          [-8.7789e+00, -1.2857e+01, -6.5494e+00,  ..., -3.2747e+01,\n",
      "           -6.8454e+01, -6.5551e+01]],\n",
      "\n",
      "         [[ 6.9078e-01, -5.7994e+00, -9.5615e+00,  ..., -1.1897e+01,\n",
      "           -3.9974e+00, -1.6838e+01],\n",
      "          [-5.2018e+01, -8.6139e+01, -1.0573e+02,  ..., -2.6175e+01,\n",
      "           -1.4434e+01, -2.2894e+01],\n",
      "          [-2.8936e+01, -4.0475e+01, -5.1596e+01,  ..., -8.9696e+00,\n",
      "           -7.0021e+00, -1.2463e+01],\n",
      "          ...,\n",
      "          [-4.0759e+01, -6.4417e+01, -5.5947e+01,  ..., -7.3409e+01,\n",
      "           -1.2904e+02, -6.9083e+01],\n",
      "          [-2.0514e+01, -4.6923e+01, -7.3574e+01,  ..., -6.6204e+01,\n",
      "           -1.0791e+02, -8.7350e+01],\n",
      "          [-4.4967e+01, -5.5078e+01, -5.8288e+01,  ..., -5.0588e+01,\n",
      "           -7.7905e+01, -3.5838e+01]],\n",
      "\n",
      "         [[ 1.7015e+01,  1.3691e+01,  2.1204e+01,  ..., -8.6943e+00,\n",
      "           -1.2093e+01, -8.5308e+00],\n",
      "          [ 1.2841e+01,  8.6539e-01,  1.1064e-01,  ...,  1.2123e+00,\n",
      "           -4.9720e+00, -4.7216e+00],\n",
      "          [ 1.4056e+01,  9.5987e+00, -5.3632e-01,  ...,  3.3366e+00,\n",
      "           -3.7020e+00, -4.6144e-01],\n",
      "          ...,\n",
      "          [ 1.0859e+01, -1.1730e+01,  6.7680e+00,  ..., -8.6106e+01,\n",
      "           -7.8527e+01, -5.5841e+01],\n",
      "          [ 2.6552e+01, -1.4950e+01, -3.6624e+01,  ..., -6.8540e+01,\n",
      "           -7.1390e+01, -6.3156e+01],\n",
      "          [ 2.1624e+01,  2.1363e+01, -3.0992e-01,  ..., -1.3654e+01,\n",
      "           -7.7102e+00,  2.0817e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3818e+01, -2.1899e+00, -1.7789e+01,  ...,  2.0293e+00,\n",
      "           -2.6640e+01, -1.9618e+01],\n",
      "          [ 7.4814e-01, -4.4259e+01, -6.2167e+01,  ..., -5.9521e+01,\n",
      "           -6.6107e+01, -3.9068e+01],\n",
      "          [-6.3059e+00, -5.3671e+01, -6.7218e+01,  ..., -5.4900e+01,\n",
      "           -6.3661e+01, -5.1852e+01],\n",
      "          ...,\n",
      "          [-2.6392e+00, -4.9324e+01, -6.3670e+01,  ...,  5.2083e+00,\n",
      "            8.6700e+00,  2.7098e+00],\n",
      "          [-1.1551e+01, -7.2562e+01, -8.9832e+01,  ..., -2.2702e+01,\n",
      "            1.5126e+01,  8.5503e+00],\n",
      "          [ 1.0526e+01, -2.9476e+01, -4.5258e+01,  ..., -2.0047e+01,\n",
      "            3.2594e+00,  1.4842e+00]],\n",
      "\n",
      "         [[-4.9228e+01, -7.1733e+01, -7.7632e+01,  ..., -1.2814e+01,\n",
      "           -1.0582e+01,  2.8080e-01],\n",
      "          [-4.1566e+01, -6.0566e+01, -7.3301e+01,  ..., -2.2188e+01,\n",
      "           -1.9946e+01, -3.7634e+00],\n",
      "          [-2.1451e+01, -3.1490e+01, -4.9909e+01,  ..., -1.6525e+01,\n",
      "           -1.7426e+01, -1.6263e+01],\n",
      "          ...,\n",
      "          [-3.4382e+01, -4.6762e+01, -6.3443e+01,  ..., -4.0986e+00,\n",
      "            2.2598e+01, -5.5900e-01],\n",
      "          [-5.6650e+01, -8.1799e+01, -1.0522e+02,  ...,  5.0936e+00,\n",
      "           -3.7466e+00, -2.0585e+00],\n",
      "          [-1.6161e+01, -2.3467e+01, -3.6543e+01,  ...,  1.5165e+01,\n",
      "            2.6512e+01,  1.8422e+00]],\n",
      "\n",
      "         [[ 1.3299e+01,  2.6765e+01,  1.4122e+01,  ...,  4.9749e+00,\n",
      "           -1.5131e+00, -7.1485e+00],\n",
      "          [ 4.3791e+01,  8.0375e+01,  7.4763e+01,  ..., -5.2170e+00,\n",
      "           -1.0069e+01, -1.1809e+01],\n",
      "          [ 3.6713e+01,  7.2336e+01,  5.5624e+01,  ..., -2.4918e+01,\n",
      "           -1.8975e+01, -1.8888e+01],\n",
      "          ...,\n",
      "          [ 3.7831e+01,  8.3534e+01,  5.8876e+01,  ...,  2.2532e+01,\n",
      "            5.0562e+01,  1.2593e+01],\n",
      "          [ 2.4157e+00,  1.5982e+01,  9.0343e+00,  ...,  4.8022e+01,\n",
      "            2.9004e+01,  2.0313e+01],\n",
      "          [-7.0955e+00, -1.8654e+01, -2.9639e+01,  ...,  3.6845e+01,\n",
      "            6.0053e+01,  7.3885e+00]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "iter_start_time = time.time()\n",
    "save_dir = \"D:/VILAB/CSA_fer_data/result\"\n",
    "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):   \n",
    "    epoch_start_time = time.time()\n",
    "    epoch_iter = 0\n",
    "\n",
    "#     image, mask, gt = [x.cuda() for x in next(iterator_train)]\n",
    "    for image, mask in (iterator_train):\n",
    "        image=image.cuda()\n",
    "        mask=mask.cuda()\n",
    "        mask=mask[0][0]\n",
    "        mask=torch.unsqueeze(mask,0)\n",
    "        mask=torch.unsqueeze(mask,1)\n",
    "        mask=mask.byte()\n",
    "        total_steps += opt.batchSize\n",
    "        epoch_iter +=opt.batchSize\n",
    "        model.set_input(image,mask) # it not only sets the input data with mask, but also sets the latent mask.\n",
    "        model.set_gt_latent()\n",
    "        model.optimize_parameters()\n",
    "        break\n",
    "        if total_steps %opt.display_freq== 0:\n",
    "            real_A,real_B,fake_B=model.get_current_visuals()\n",
    "            #real_A=input, real_B=ground truth fake_b=output\n",
    "            pic = (torch.cat([real_A, real_B,fake_B], dim=0) + 1) / 2.0\n",
    "            torchvision.utils.save_image(pic, '%s/Epoch_(%d)_(%dof%d).jpg' % (\n",
    "            save_dir, epoch, total_steps + 1, len(dataset_train)), nrow=2)\n",
    "        if total_steps %1== 0:\n",
    "            errors = model.get_current_errors()\n",
    "            t = (time.time() - iter_start_time) / opt.batchSize\n",
    "            print(errors)\n",
    "    break\n",
    "    if epoch % opt.save_epoch_freq == 0:\n",
    "        print('saving the model at the end of epoch %d, iters %d' %(epoch, total_steps))\n",
    "        model.save(epoch)\n",
    "\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "            (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "\n",
    "    model.update_learning_rate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "GAN",
   "language": "python",
   "name": "gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
